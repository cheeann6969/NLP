# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gk6U9eLzBk_or54tPTV3ssbDmVwqdeej

#### LIBRARIES
"""

import re
import os
import json
import pickle 
import datetime
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder 
from sklearn.model_selection import train_test_split
from sklearn.metrics import \
  classification_report, confusion_matrix, ConfusionMatrixDisplay


from tensorflow.keras import Input,Sequential
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from tensorflow.keras.layers import \
  LSTM, Dense,Dropout, Dropout, Embedding, Bidirectional

# CONSTANTS
OHE_SAVE_PATH = os.path.join(os.getcwd(),'models','ohe.pkl')
MODEL_SAVE_PATH = os.path.join(os.getcwd(),'models','model.h5')
TOKENIZER_SAVE_PATH = os.path.join(os.getcwd(),'models','tokenizer.json')
LOGS_PATH = os.path.join(os.getcwd(),'logs',datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
CSV_PATH = 'https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv'

"""#%% 1) Data Loading"""

df = pd.read_csv(CSV_PATH)

"""#%% 2) Data Inspection"""

df.head()

df.describe().T

#checking for imbalance datasets
display(df.groupby('category',dropna=False)['category'].size())

df[df.duplicated()].shape

#df = df[~df.duplicated()]
#df[df.duplicated()].shape

#drop duplicates and recheck

df.isna().sum()

print(df['text'][4])

print(df['text'][10])

"""#%% 3) Data cleaning 
to remove symbols and html tags
"""

text = df['text']
category = df['category']

text_backup = text.copy()
category_backup = category.copy()

for index, txt in enumerate(text):
    text[index] = re.sub('<.*?>','',txt) 
    text[index] = re.sub('[^a-zA-Z]',' ',txt).lower().split()
    text[index] = re.sub('[0-9]','',txt)

text_backup = text.copy()
category_backup = category.copy()

"""#%% 4) Features Selection 
skip for this dataset

#%% 5) Data preprocessing
"""

# X Features
vocab_size = 10000
oov_token = '<OOV>'

tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_token)

tokenizer.fit_on_texts(text)  # to learn 
word_index = tokenizer.word_index

print(dict(list(word_index.items())[0:10]))  # to slice the data, show only 10

display(dict(list(word_index.items())))   # to show all

text_int = tokenizer.texts_to_sequences(text) # To convert into numbers

max_len = np.median([len(text_int[i]) for i in range(len(text_int))])

padded_text = pad_sequences(text_int,
                            maxlen=int(max_len),
                            padding='post',
                            truncating='post')

#Y target
ohe = OneHotEncoder(sparse=False)
category = ohe.fit_transform(np.expand_dims(category,axis=-1))

X_train,X_test,y_train,y_test = train_test_split(padded_text,category,
                                               test_size=0.3,
                                               random_state=123)

"""#%% 6) Model Development """

input_shape = np.shape(X_train)[1:]
out_dim =128
nb_class = len(np.unique(y_train,axis=0))

model=Sequential()
model.add(Input(shape=(input_shape)))
model.add(Embedding(vocab_size,out_dim))
model.add(Bidirectional(LSTM(128,return_sequences=True)))
model.add(Dropout(0.3))
model.add(Bidirectional(LSTM(128)))
model.add(Dropout(0.3))
model.add(Dense(nb_class,activation='softmax'))
model.summary()

model.compile(optimizer = 'adam', 
              loss='categorical_crossentropy',
              metrics=['acc'])

plot_model(model,show_shapes=(True))

tensorboard_callback=TensorBoard(log_dir=LOGS_PATH, histogram_freq=1)

early_callback = EarlyStopping(monitor = 'val_loss', patience=5)


hist = model.fit(X_train, y_train, 
                 epochs=20,
                 validation_data=(X_test, y_test),
                 callbacks=[tensorboard_callback,early_callback])

print(hist.history.keys())

plt.figure()
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.xlabel('epoch')
plt.legend(['Training Loss','Validation Loss'])
plt.rcParams['figure.figsize'] = (9, 6)
plt.show()

plt.figure()
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.xlabel('epoch')
plt.legend(['Training Acc','Validation Acc'])
plt.rcParams['figure.figsize'] = (9, 6)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %load_extÂ tensorboard
# %tensorboard --logdir logs

"""#%% 7) Model Evaluation"""

#labels = model.classes_

# model analysis 
labels = ['tech','business','sport','entertainment','politics']

y_pred = np.argmax(model.predict(X_test),axis=1)
y_true = np.argmax(y_test, axis=1)

cr = classification_report(y_true, y_pred, target_names = labels)
print(cr)

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix = cm ,display_labels =labels)
disp.plot(cmap='Blues')
plt.rcParams['figure.figsize'] = [7, 7]
plt.show()

"""#%% 8) Model Saving"""

token_json = tokenizer.to_json()

with open(TOKENIZER_SAVE_PATH,'w') as file: 
    json.dump(token_json,file)

with open(OHE_SAVE_PATH,'wb') as file: 
    pickle.dump(ohe,file)

# model saving

model.save(MODEL_SAVE_PATH)